Dylan Chow

Problem 1
 initially started modifying my IndexEngine by adding a function and placing all of my edits in the function.  I provided arguments for the deque data structure with the entire article's data which I had originally used to store the files individually in HW Assignment 1 and the document's internal ID.  Originally, I just Frankenstein'd my code from the previous assignment, which was ridiculously inefficient and unresourceful.  I had this long set of nest while loops and nested if-statements, which worked accorsing to what I saw when I printed the output in the terminal.  I found that the code would run for hours and would process about 10000 articles per hour (which is obviously pretty bad for a dataset with 130000+ articles, but it's worth mentioning that I was using my high-end desktop at home over reading week and it was STILL that slow).  This may have been for two reasons.  
For one, I had originally coded the if-statement to detect the "TEXT", "GRAPHIC" and "HEADLINE" segments of an article and treat them separately, which was not ideal because the code in each case was the same anyway with the exception of the text of the divider for the end of the section.  I realized that the or operand exists, so I just combined the statements by continuing the loop if the line does not start with "</" or the text between the <> is "/P" (since just having the first condition omits terms from TEXT segments with multiple paragraphs).
The second reason my code was slow was because I was doing the term separation without a library.  I originally wrote the code to manually separate each term using nested while loops and several if statements.  This obviously killed the run time as this process was constantly updating local variables, frequently calling libraries, and there were millions of terms in the dataset, so who knows how many times that loop had to run?  I eventually added NLTK (Native Languaging Toolkit) to my program for its tokenizing functions which bundled the tokenizing process for all terms in a given line at once.  (I don't know if I lose marks if it doesn't work for whoever is marking it or if I don't mention this, but you have to get the library and then the 'punkt' package from the library too.) I gathered code from other sources to understand how to downcase each term and remove the punctution from each term to have a unsorted list of all terms in (the relevant parts of) a given article.
I then created the set of term frequencies in the document through a loop.  This was mostly taking advantage of the count() function of deque data structures in Python as this saved me a loop from attaining the term frequencies for each term in the doc.  Now that I had the term frequency, I just had to make sure I did not already put it in the term frequency array I had for the document and to append it in the array if I the term was not already in there.
Finally was one last loop to add any new terms to the lexicon and to add the document term frequencies into the inverted index.  The if-statement checks if the lexicon's can find any element for the given term.  If the lexicon can't find the term (or if it's empty like for the first term being added to the lexicon), it adds it to the lexicon.  My lexicon is a one dimensional array and uses the index as the term ID because I thought it would be easier.  After the loop ensures the term is in the lexicon, the document ID and the term frequency are added to the array of the corresponding term ID in the inverted index.
After the entire code finishes, the arrays for the lexicon and inverted index are saved in .npy format.  This is just because I looked online and found it was the fastest (and smallest uncompressed) format to store an array in.
https://i.stack.imgur.com/4d6yo.png

P.S. I forgot to implement the word count per document, so I added a couple of lines to get the length of wordQ (the deque I used to collect all the terms in a given doc) and it got about 10-20 times slower.  So, I structured the code from the function to fit into what I had from HW Assignment 1.  I then made a bunch of other optimizations because it was still being slow, (less slow, but still extremely slow).  Just note that the above explanation is what was my code prior to the optimizations and I made several changes to the file.

Problem 2